{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZEWbJhKjnn5_"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morutkin/MLProject/blob/master/final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beUnJ-IiRdHH",
        "colab_type": "text"
      },
      "source": [
        "2 Step Character Translation Pipeline \n",
        "\n",
        "Step 1: Detection \n",
        "* Detecting the average character size per image using CenterNet \n",
        "* Using the above model to then create the bounding boxes with character location prediction \n",
        "\n",
        "Step 2: Classification \n",
        "* Using CNN with ResNet Backbone for classifying the characters with unicode\n",
        "\n",
        "Backbone of this process of creating a CenterNet is attributed to K_Mat's Kernel on Kaggle \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvtHt7D1lUSW",
        "colab_type": "text"
      },
      "source": [
        "### Connect To Google Drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efXh4Q4hlTv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from googleapiclient.discovery import build\n",
        "import io, os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "results = drive_service.files().list(\n",
        "        q=\"name = 'kaggle.json'\", fields=\"files(id)\").execute()\n",
        "kaggle_api_key = results.get('files', [])\n",
        "filename = \"/content/.kaggle/kaggle.json\"\n",
        "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
        "request = drive_service.files().get_media(fileId=kaggle_api_key[0]['id'])\n",
        "fh = io.FileIO(filename, 'wb')\n",
        "downloader = MediaIoBaseDownload(fh, request)\n",
        "done = False\n",
        "while done is False:\n",
        "    status, done = downloader.next_chunk()\n",
        "    print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "os.chmod(filename, 600)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqXlGFgWlspC",
        "colab_type": "text"
      },
      "source": [
        "### Install Kuzushiji Kaggle Sets On The Drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGZUjPxnlpaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9DSXDhklxvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17xdhSn5lzVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kaggle competitions download  -c kuzushiji-recognition -p /content/kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0ir8943mAv8",
        "colab_type": "text"
      },
      "source": [
        "### Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PS_k_eBmCXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import pandas as pd\n",
        "from PIL import Image, ImageDraw\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.io.json import json_normalize\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import KFold,train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "from tensorflow.keras import backend as K \n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "import os  \n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "from keras.objectives import mean_squared_error\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "#import mmcv\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensor\n",
        "import torch.utils.data\n",
        "#from mmdet.core.bbox import bbox_overlaps\n",
        "import cv2\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdv3NYgGmUuD",
        "colab_type": "text"
      },
      "source": [
        "### Install OpenCV + Albumenations + Keras + Tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB7jffPhmZ06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install opencv-python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO0_Tk-kmbDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install --upgrade tqdm opencv-python==3.4.5.20 timm --user"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hczTAZ6lmcus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install --upgrade git+https://github.com/albu/albumentations --user"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "latLe1cXmufE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install Keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTKAJrJomwcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install Tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-vK-uxVmykm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0 tensorflow-addons"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C-nGbBFnLT4",
        "colab_type": "text"
      },
      "source": [
        "## Data Unpacking "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdwJPkyRnQ4i",
        "colab_type": "text"
      },
      "source": [
        "### Unzipping Files + File Placements "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qudDwqp8nVI-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#unzip pictures in train \n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/kaggle/train_images.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/kaggle/train_images')\n",
        "\n",
        "with zipfile.ZipFile('/content/kaggle/test_images.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/kaggle/test_images')\n",
        "\n",
        "with zipfile.ZipFile('/content/kaggle/train.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/kaggle/train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J07TvQPDnY1x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data frame of [\"100241706_00004_2\", U+309D 1076 3545 31 71 ...]\n",
        "#second part is unicode, x, y, w, h for all chars \n",
        "train_set = pd.read_csv('/content/kaggle/train.csv.zip', compression='zip', header=0, sep=',', quotechar='\"', keep_default_na=False)\n",
        "train_set=train_set.dropna(axis=0, how='any')\n",
        "train_set=train_set.reset_index(drop=True)\n",
        "#print(train_set.head())\n",
        "#print(train_set[\"labels\"])\n",
        "\n",
        "# data frame of [\"U+0031\", \"𠀋\"]\n",
        "unimap = pd.read_csv('/content/kaggle/unicode_translation.csv')\n",
        "sample_submiss = pd.read_csv('/content/kaggle/sample_submission.csv')\n",
        "\n",
        "#image directories \n",
        "img_dir = Path('/content/kaggle/train_images')\n",
        "img_dirTest = Path('/content/kaggle/test_images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xv5OSYQHnlv6",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Processing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEWbJhKjnn5_",
        "colab_type": "text"
      },
      "source": [
        "### Image Cropping "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNvi_6w3nqJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cut each character with corresponding unicode translated label\n",
        "def cropImage(labels, loop1_index, img_dir):\n",
        "  \"\"\"\n",
        "  reading images from train: imread\n",
        "  a pixel array in BGR format \n",
        "\n",
        "  [[[ 72  99 143]\n",
        "    [ 76 103 147]\n",
        "    [ 78 106 147]\n",
        "    ..., \n",
        "    [159 186 207]\n",
        "    [160 187 213]\n",
        "    [157 187 212]]\n",
        "  \"\"\"\n",
        "  img = cv2.imread(im_dir)\n",
        "  # convert image to RGB color for matplotlib\n",
        "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "  labels = labels.split(\" \")\n",
        "\n",
        "  #maps {unicode -> x, y, w, h}\n",
        "  labels[::5] = map(unicodeToInt, labels[::5])\n",
        "  unicode = labels[::5]\n",
        "  print(unicode)\n",
        "  del labels[::5] # deletes the one thats captured \n",
        "\n",
        "  labels = np.array(labels, dtype=np.int16)\n",
        "  labels = labels.reshape(-1,4)\n",
        "\n",
        "  labels[:, 2] = np.sum(a=labels[:,[0,2]], axis=1)\n",
        "  labels[:, 3] = np.sum(a=labels[:,[1,3]], axis=1)\n",
        "\n",
        "  [Image.fromarray(img[label[1]:label[3], label[0]:label[2]]).save(f\"{CHARS}/{unicode[loop2_index]}_{loop1_index}-{loop2_index}.jpg\") for loop2_index,label in enumerate(labels)]\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fFGaMnonyTS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RandomCrop(image: Image.Image) -> Image.Image:\n",
        "  img_w, img_h = image.size\n",
        "  assert 0.5 < scale_range[0] < scale_range[1] <= 1\n",
        "  self.scale_range = scale_range\n",
        "  self.translation_range = translation_range\n",
        "  self.size = size\n",
        "  crop_w = int(img_w * np.random.uniform(*self.scale_range))\n",
        "  crop_h = int(img_h * np.random.uniform(*self.scale_range))\n",
        "  dx = int(round(img_w * np.random.uniform(*self.translation_range) / 1.4))\n",
        "  dy = int(round(img_h * np.random.uniform(*self.translation_range) / 1.4))\n",
        "  x = (img_w - crop_w) // 2 + dx\n",
        "  y = (img_h - crop_h) // 2 + dy\n",
        "  cropped = image.crop((x, y, x + crop_w, y + crop_h))\n",
        "  resized = cropped.resize(self.size, resample=Image.BILINEAR)\n",
        "  return resized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lXmLoAtn90X",
        "colab_type": "text"
      },
      "source": [
        "## Character size distribution\n",
        "\n",
        "We want to predict the average letter size in order to know where to look for when creating the bounding boxes, we're using a Centernet backbone modeled off of K_mat in the Kaggle Competition "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skZUQeIyoBdz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_1=\"/content/kaggle/train.csv\"\n",
        "path_2=\"/content/kaggle/train_images/\"\n",
        "path_3=\"/content/kaggle/test_images/\"\n",
        "path_4=\"/content/kaggle/sample_submission.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJrRsgD5oCWA",
        "colab_type": "text"
      },
      "source": [
        "Making train into a DF + preprocessing by dropping NAs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwqzpbkvoldy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#train data \n",
        "df_train=pd.read_csv('/content/kaggle/train.csv.zip', compression='zip', header=0, sep=',', quotechar='\"')\n",
        "df_train=df_train.dropna(axis=0, how='any') #you can use nan data(page with no letter)\n",
        "df_train=df_train.reset_index(drop=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZjOt2ZJorAD",
        "colab_type": "text"
      },
      "source": [
        "making a:\n",
        "\n",
        "* category_names: unicode names\n",
        "* dict_cat\n",
        "* inv_dict_cat\n",
        "* annotation_list_train\n",
        "[pic_path_name, path to cat# and annotations [x,y,w,h]]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JdbWyf6oor8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annotation_list_train=[]\n",
        "category_names=set()\n",
        "\n",
        "for i in range(len(df_train)):\n",
        "  ann=np.array(df_train.loc[i,\"labels\"].split(\" \")).reshape(-1,5) # cat,x,y,width,height for each picture\n",
        "  category_names=category_names.union({i for i in ann[:,0]})\n",
        "\n",
        "#just the existing unicode labels \n",
        "print(\"category_names\")  \n",
        "print(category_names)\n",
        "\n",
        "category_names=sorted(category_names)\n",
        "dict_cat={list(category_names)[j]:str(j) for j in range(len(category_names))}\n",
        "# {unicode: placement # }\n",
        "print(\"dict_cat\")  \n",
        "print(dict_cat)\n",
        "\n",
        "inv_dict_cat={str(j):list(category_names)[j] for j in range(len(category_names))}\n",
        "# {placement #: unicode }\n",
        "print(\"inv_dict_cat\")  \n",
        "print(inv_dict_cat)\n",
        "\n",
        "  \n",
        "for i in range(len(df_train)):\n",
        "  ann=np.array(df_train.loc[i,\"labels\"].split(\" \")).reshape(-1,5)#cat,left,top,width,height for each picture\n",
        "  for j,category_name in enumerate(ann[:,0]):\n",
        "    ann[j,0]=int(dict_cat[category_name])  \n",
        "  ann=ann.astype('int32')\n",
        "  ann[:,1]+=ann[:,3]//2 #center_x\n",
        "  ann[:,2]+=ann[:,4]//2 #center_y\n",
        "  annotation_list_train.append([\"{}{}.jpg\".format(path_2,df_train.loc[i,\"image_id\"]),ann])\n",
        "# annotation_list_train contains [path-to-pic, list of cat# bottom top width height ]\n",
        "# annotation_list_train [0][0] = path to picture \n",
        "# annotation_list_train [0][1] = path to cat# and annotations  \n",
        "\n",
        "\n",
        "print(\"sample image\")\n",
        "input_width,input_height=1024, 1024\n",
        "img = np.asarray(Image.open(annotation_list_train[0][0]).resize((input_width,input_height)).convert('RGB'))\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjObaxjQpCv4",
        "colab_type": "text"
      },
      "source": [
        "get directory of test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceQMgArGpDQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get directory of test images\n",
        "df_submission=pd.read_csv(path_4)\n",
        "id_test=path_3+df_submission[\"image_id\"]+\".jpg\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZprQT5e8JiCG",
        "colab_type": "text"
      },
      "source": [
        "Making a directory of the average letter sizes for each image according to how many supposed labels are in each picture. \n",
        "\n",
        "We then save the [image path, log of average letter size] in the resized/ directory  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aizdnIqQpFCX",
        "colab_type": "code",
        "outputId": "dc58e020-822e-46ea-d743-fb04cb944f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "aspect_ratio_pic_all=[]\n",
        "aspect_ratio_pic_all_test=[]\n",
        "average_letter_size_all=[]\n",
        "train_input_for_size_estimate=[]\n",
        "resize_dir=\"resized/\"\n",
        "\n",
        "#make a directory of the resized images size estimates  \n",
        "if os.path.exists(resize_dir) == False:os.mkdir(resize_dir)\n",
        "\n",
        "for i in range(len(annotation_list_train)):\n",
        "    with Image.open(annotation_list_train[i][0]) as f:\n",
        "        width,height=f.size\n",
        "        area=width*height\n",
        "        aspect_ratio_pic=height/width\n",
        "        aspect_ratio_pic_all.append(aspect_ratio_pic)\n",
        "        letter_size=annotation_list_train[i][1][:,3]*annotation_list_train[i][1][:,4]\n",
        "        letter_size_ratio=letter_size/area\n",
        "    \n",
        "        average_letter_size=np.mean(letter_size_ratio)\n",
        "        average_letter_size_all.append(average_letter_size)\n",
        "        train_input_for_size_estimate.append([annotation_list_train[i][0],np.log(average_letter_size)])\n",
        "    \n",
        "\n",
        "for i in range(len(id_test)):\n",
        "    with Image.open(id_test[i]) as f:\n",
        "        width,height=f.size\n",
        "        aspect_ratio_pic=height/width\n",
        "        aspect_ratio_pic_all_test.append(aspect_ratio_pic)\n",
        "\n",
        "\n",
        "plt.hist(np.log(average_letter_size_all),bins=100)\n",
        "plt.title('log(ratio of letter_size to picture_size))',loc='center',fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEICAYAAAC6fYRZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzKxgDnkzs4I55M3MCvb/4aoA5xRSECAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh91VWhxpg29",
        "colab_type": "text"
      },
      "source": [
        "* Random crop of images within the random range of 70-100% of its original size \n",
        "* converts image to RGB which is easier to work with using Tensorflow \n",
        "* outputs array of the new image dimentions \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_CsgifgpO7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "category_n=1\n",
        "import cv2\n",
        "input_width,input_height=512, 512\n",
        "\n",
        "def Datagen_sizecheck_model(filenames, batch_size, size_detection_mode=True, is_train=True,random_crop=True):\n",
        "  x=[]\n",
        "  y=[]\n",
        "  \n",
        "  count=0\n",
        "\n",
        "  while True:\n",
        "    for i in range(len(filenames)):\n",
        "      if random_crop:\n",
        "        crop_ratio=np.random.uniform(0.7,1)\n",
        "      else:\n",
        "        crop_ratio=1\n",
        "      with Image.open(filenames[i][0]) as f:\n",
        "        #random crop\n",
        "        if random_crop and is_train:\n",
        "          pic_width,pic_height=f.size\n",
        "          f=np.asarray(f.convert('RGB'),dtype=np.uint8)\n",
        "          top_offset=np.random.randint(0,pic_height-int(crop_ratio*pic_height))\n",
        "          left_offset=np.random.randint(0,pic_width-int(crop_ratio*pic_width))\n",
        "          bottom_offset=top_offset+int(crop_ratio*pic_height)\n",
        "          right_offset=left_offset+int(crop_ratio*pic_width)\n",
        "          f=cv2.resize(f[top_offset:bottom_offset,left_offset:right_offset,:],(input_height,input_width))\n",
        "        else:\n",
        "          f=f.resize((input_width, input_height))\n",
        "          f=np.asarray(f.convert('RGB'),dtype=np.uint8)          \n",
        "        x.append(f)\n",
        "      \n",
        "      \n",
        "      if random_crop and is_train:\n",
        "        y.append(filenames[i][1]-np.log(crop_ratio))\n",
        "      else:\n",
        "        y.append(filenames[i][1])\n",
        "      \n",
        "      count+=1\n",
        "      if count==batch_size:\n",
        "        x=np.array(x, dtype=np.float32)\n",
        "        y=np.array(y, dtype=np.float32)\n",
        "\n",
        "        inputs=x/255\n",
        "        targets=y       \n",
        "        x=[]\n",
        "        y=[]\n",
        "        count=0\n",
        "        yield inputs, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Sem6sSLqSGy",
        "colab_type": "text"
      },
      "source": [
        "### CenterNet For Ideal Image Size "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cz1Y91pqWUJ",
        "colab_type": "text"
      },
      "source": [
        "#### Blocks of layers for CenterNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRe_n4rGqQDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def aggregation_block(x_shallow, x_deep, deep_ch, out_ch):\n",
        "  x_deep= Conv2DTranspose(deep_ch, kernel_size=2, strides=2, padding='same', use_bias=False)(x_deep)\n",
        "  x_deep = BatchNormalization()(x_deep)   \n",
        "  x_deep = LeakyReLU(alpha=0.1)(x_deep)\n",
        "  x = Concatenate()([x_shallow, x_deep])\n",
        "  x=Conv2D(out_ch, kernel_size=1, strides=1, padding=\"same\")(x)\n",
        "  x = BatchNormalization()(x)   \n",
        "  x = LeakyReLU(alpha=0.1)(x)\n",
        "  return x\n",
        "\n",
        "def cbr(x, out_layer, kernel, stride):\n",
        "  x=Conv2D(out_layer, kernel_size=kernel, strides=stride, padding=\"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = LeakyReLU(alpha=0.1)(x)\n",
        "  return x\n",
        "\n",
        "def resblock(x_in,layer_n):\n",
        "  x=cbr(x_in,layer_n,3,1)\n",
        "  x=cbr(x,layer_n,3,1)\n",
        "  x=Add()([x,x_in])\n",
        "  return x  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRXMaAkkqjmP",
        "colab_type": "text"
      },
      "source": [
        "### Creating the model \n",
        "\n",
        "The ENCODER:\n",
        "* extracts features and reduces dimentions through each layer \n",
        "\n",
        "The DECODER:\n",
        "* inverts the process and reverts the layers into being interpretable again\n",
        "\n",
        "This is the literal CenterNet  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHn6iMq0qm_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(input_shape, size_detection_mode=True, aggregation=True):\n",
        "    input_layer = Input(input_shape)\n",
        "    \n",
        "    #resized input\n",
        "    input_layer_1=AveragePooling2D(2)(input_layer)\n",
        "    input_layer_2=AveragePooling2D(2)(input_layer_1)\n",
        "\n",
        "    #### ENCODER ####\n",
        "\n",
        "    x_0= cbr(input_layer, 16, 3, 2)#512->256\n",
        "    concat_1 = Concatenate()([x_0, input_layer_1])\n",
        "\n",
        "    x_1= cbr(concat_1, 32, 3, 2)#256->128\n",
        "    concat_2 = Concatenate()([x_1, input_layer_2])\n",
        "\n",
        "    x_2= cbr(concat_2, 64, 3, 2)#128->64\n",
        "    \n",
        "    x=cbr(x_2,64,3,1)\n",
        "    x=resblock(x,64)\n",
        "    x=resblock(x,64)\n",
        "    \n",
        "    x_3= cbr(x, 128, 3, 2)#64->32\n",
        "    x= cbr(x_3, 128, 3, 1)\n",
        "    x=resblock(x,128)\n",
        "    x=resblock(x,128)\n",
        "    x=resblock(x,128)\n",
        "    \n",
        "    x_4= cbr(x, 256, 3, 2)#32->16\n",
        "    x= cbr(x_4, 256, 3, 1)\n",
        "    x=resblock(x,256)\n",
        "    x=resblock(x,256)\n",
        "    x=resblock(x,256)\n",
        "    x=resblock(x,256)\n",
        "    x=resblock(x,256)\n",
        " \n",
        "    x_5= cbr(x, 512, 3, 2)#16->8\n",
        "    x= cbr(x_5, 512, 3, 1)\n",
        "    \n",
        "    x=resblock(x,512)\n",
        "    x=resblock(x,512)\n",
        "    x=resblock(x,512)\n",
        "    \n",
        "    if size_detection_mode:\n",
        "      x=GlobalAveragePooling2D()(x)\n",
        "      x=Dropout(0.2)(x)\n",
        "      out=Dense(1,activation=\"linear\")(x)\n",
        "    \n",
        "    else:#centernet mode\n",
        "    #### DECODER ####\n",
        "      x_1= cbr(x_1, output_layer_n, 1, 1)\n",
        "      x_1 = aggregation_block(x_1, x_2, output_layer_n, output_layer_n)\n",
        "      x_2= cbr(x_2, output_layer_n, 1, 1)\n",
        "      x_2 = aggregation_block(x_2, x_3, output_layer_n, output_layer_n)\n",
        "      x_1 = aggregation_block(x_1, x_2, output_layer_n, output_layer_n)\n",
        "      x_3= cbr(x_3, output_layer_n, 1, 1)\n",
        "      x_3 = aggregation_block(x_3, x_4, output_layer_n, output_layer_n) \n",
        "      x_2 = aggregation_block(x_2, x_3, output_layer_n, output_layer_n)\n",
        "      x_1 = aggregation_block(x_1, x_2, output_layer_n, output_layer_n)\n",
        "      \n",
        "      x_4= cbr(x_4, output_layer_n, 1, 1)\n",
        "\n",
        "      x=cbr(x, output_layer_n, 1, 1)\n",
        "      x= UpSampling2D(size=(2, 2))(x)#8->16 tconvのがいいか\n",
        "\n",
        "      x = Concatenate()([x, x_4])\n",
        "      x=cbr(x, output_layer_n, 3, 1)\n",
        "      x= UpSampling2D(size=(2, 2))(x)#16->32\n",
        "    \n",
        "      x = Concatenate()([x, x_3])\n",
        "      x=cbr(x, output_layer_n, 3, 1)\n",
        "      x= UpSampling2D(size=(2, 2))(x)#32->64   128のがいいかも？ \n",
        "    \n",
        "      x = Concatenate()([x, x_2])\n",
        "      x=cbr(x, output_layer_n, 3, 1)\n",
        "      x= UpSampling2D(size=(2, 2))(x)#64->128 \n",
        "      \n",
        "      x = Concatenate()([x, x_1])\n",
        "      x=Conv2D(output_layer_n, kernel_size=3, strides=1, padding=\"same\")(x)\n",
        "      out = Activation(\"sigmoid\")(x)\n",
        "    \n",
        "    model=Model(input_layer, out)\n",
        "    \n",
        "    return model\n",
        "\n",
        "def model_fit_sizecheck_model(model,train_list,cv_list,n_epoch,batch_size=32):\n",
        "    hist = model.fit_generator(\n",
        "        Datagen_sizecheck_model(train_list,batch_size, is_train=True,random_crop=True),\n",
        "        steps_per_epoch = len(train_list) // batch_size,\n",
        "        epochs = n_epoch,\n",
        "        validation_data=Datagen_sizecheck_model(cv_list,batch_size, is_train=False,random_crop=False),\n",
        "        validation_steps = len(cv_list) // batch_size,\n",
        "        callbacks = [lr_schedule, model_checkpoint],#[early_stopping, reduce_lr, model_checkpoint],\n",
        "        shuffle = True,\n",
        "        verbose = 1\n",
        "    )\n",
        "    return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXNBZsdXrBMn",
        "colab_type": "text"
      },
      "source": [
        "Create the instance of the Model with the following params + save weights  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBZrz1NZrVth",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "model=create_model(input_shape=(input_height,input_width,3),size_detection_mode=True)\n",
        "\n",
        "def lrs(epoch):\n",
        "    lr = 0.0005\n",
        "    if epoch>10:\n",
        "        lr = 0.0001\n",
        "    return lr\n",
        "\n",
        "\n",
        "lr_schedule = LearningRateScheduler(lrs)\n",
        "model_checkpoint = ModelCheckpoint(\"final_weights_step1.hdf5\", monitor = 'val_loss', verbose = 1,\n",
        "                                      save_best_only = True, save_weights_only = True, period = 1)\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjgPRXCprh3G",
        "colab_type": "code",
        "outputId": "c22bc4fb-57a8-4c82-f31d-a89b0a4d1bb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "train_list, cv_list = train_test_split(train_input_for_size_estimate, random_state = 111,test_size = 0.2)\n",
        "\n",
        "\n",
        "learning_rate=0.0005\n",
        "n_epoch=3\n",
        "batch_size=13\n",
        "\n",
        "model.compile(loss=mean_squared_error, optimizer=Adam(lr=learning_rate))\n",
        "hist = model_fit_sizecheck_model(model,train_list,cv_list,n_epoch,batch_size)\n",
        "\n",
        "#model.save_weights('final_weights_step1.h5')\n",
        "model.load_weights('final_weights_step1.hdf5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "220/221 [============================>.] - ETA: 1s - loss: 3.9972Epoch 1/3\n",
            " 55/221 [======>.......................] - ETA: 3:47 - loss: 13.0995\n",
            "Epoch 00001: val_loss improved from inf to 13.09954, saving model to final_weights_step1.hdf5\n",
            "221/221 [==============================] - 414s 2s/step - loss: 3.9850 - val_loss: 13.0995\n",
            "Epoch 2/3\n",
            "220/221 [============================>.] - ETA: 1s - loss: 0.7063Epoch 1/3\n",
            " 55/221 [======>.......................] - ETA: 3:33 - loss: 0.1845\n",
            "Epoch 00002: val_loss improved from 13.09954 to 0.18455, saving model to final_weights_step1.hdf5\n",
            "221/221 [==============================] - 380s 2s/step - loss: 0.7048 - val_loss: 0.1845\n",
            "Epoch 3/3\n",
            "220/221 [============================>.] - ETA: 1s - loss: 0.6492Epoch 1/3\n",
            " 55/221 [======>.......................] - ETA: 3:33 - loss: 0.3856\n",
            "Epoch 00003: val_loss did not improve from 0.18455\n",
            "221/221 [==============================] - 375s 2s/step - loss: 0.6529 - val_loss: 0.3856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYUK3ILjtCRg",
        "colab_type": "text"
      },
      "source": [
        "### Results For Letter Size \n",
        "Based on the detector, the split size is determined for each picture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZFk9q9Usx_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict = model.predict_generator(Datagen_sizecheck_model(cv_list,batch_size, is_train=False,random_crop=False),\n",
        "                                  steps=len(cv_list) // batch_size)\n",
        "target=[cv[1] for cv in cv_list]\n",
        "plt.scatter(predict,target[:len(predict)])\n",
        "plt.title('---letter_size/picture_size--- estimated vs target ',loc='center',fontsize=10)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8aM9eRiO-d0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size=1\n",
        "predict_train = model.predict_generator(Datagen_sizecheck_model(train_input_for_size_estimate,batch_size, is_train=False,random_crop=False, ),\n",
        "                                  steps=len(train_input_for_size_estimate)//batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWMAGojqtMKs",
        "colab_type": "text"
      },
      "source": [
        "### CenterNet based off of the image detection sizes are determined for the ideal cropping of the pictures. The CenterNet used to detect character sizes below takes it into consideration "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XDW_ByGPYxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "category_n=1\n",
        "output_layer_n=category_n+4\n",
        "output_height,output_width=128,128\n",
        "​\n",
        "i=0\n",
        "​\n",
        "h_split=annotation_list_train_w_split[i][2]\n",
        "w_split=annotation_list_train_w_split[i][3]\n",
        "max_crop_ratio_h=1/h_split\n",
        "max_crop_ratio_w=1/w_split\n",
        "crop_ratio=np.random.uniform(0.5,1)\n",
        "crop_ratio_h=max_crop_ratio_h*crop_ratio\n",
        "crop_ratio_w=max_crop_ratio_w*crop_ratio\n",
        "​\n",
        "with Image.open(annotation_list_train_w_split[i][0]) as f:\n",
        "        \n",
        "        #random crop\n",
        "        pic_width,pic_height=f.size\n",
        "        f=np.asarray(f.convert('RGB'),dtype=np.uint8)\n",
        "        top_offset=np.random.randint(0,pic_height-int(crop_ratio_h*pic_height))\n",
        "        left_offset=np.random.randint(0,pic_width-int(crop_ratio_w*pic_width))\n",
        "        bottom_offset=top_offset+int(crop_ratio_h*pic_height)\n",
        "        right_offset=left_offset+int(crop_ratio_w*pic_width)\n",
        "        img=cv2.resize(f[top_offset:bottom_offset,left_offset:right_offset,:],(input_height,input_width))\n",
        "​\n",
        "      \n",
        "      \n",
        "output_layer=np.zeros((output_height,output_width,(output_layer_n+category_n)))\n",
        "for annotation in annotation_list_train_w_split[i][1]:\n",
        "​\n",
        "          x_c=(annotation[1]-left_offset)*(output_width/int(crop_ratio_w*pic_width))\n",
        "          y_c=(annotation[2]-top_offset)*(output_height/int(crop_ratio_h*pic_height))\n",
        "          width=annotation[3]*(output_width/int(crop_ratio_w*pic_width))\n",
        "          height=annotation[4]*(output_height/int(crop_ratio_h*pic_height))\n",
        "          \n",
        "          top=np.maximum(0,y_c-height/2)\n",
        "          left=np.maximum(0,x_c-width/2)\n",
        "          bottom=np.minimum(output_height,y_c+height/2)\n",
        "          right=np.minimum(output_width,x_c+width/2)\n",
        "          \n",
        "          if top>=output_height or left>=output_width or bottom<=0 or right<=0:#random crop(エリア外の除去)\n",
        "            continue\n",
        "          width=right-left\n",
        "          height=bottom-top\n",
        "          x_c=(right+left)/2\n",
        "          y_c=(top+bottom)/2\n",
        "          \n",
        "        \n",
        "        \n",
        "          category=0#not classify\n",
        "          heatmap=((np.exp(-(((np.arange(output_width)-x_c)/(width/10))**2)/2)).reshape(1,-1)\n",
        "                            *(np.exp(-(((np.arange(output_height)-y_c)/(height/10))**2)/2)).reshape(-1,1))\n",
        "          output_layer[:,:,category]=np.maximum(output_layer[:,:,category],heatmap[:,:])\n",
        "          output_layer[int(y_c//1),int(x_c//1),category_n+category]=1\n",
        "          output_layer[int(y_c//1),int(x_c//1),2*category_n]=y_c%1#height offset\n",
        "          output_layer[int(y_c//1),int(x_c//1),2*category_n+1]=x_c%1\n",
        "          output_layer[int(y_c//1),int(x_c//1),2*category_n+2]=height/output_height\n",
        "          output_layer[int(y_c//1),int(x_c//1),2*category_n+3]=width/output_width\n",
        "​\n",
        "fig, axes = plt.subplots(1, 3,figsize=(15,15))\n",
        "axes[0].set_axis_off()\n",
        "axes[0].imshow(img)\n",
        "axes[1].set_axis_off()\n",
        "axes[1].imshow(output_layer[:,:,1])\n",
        "axes[2].set_axis_off()\n",
        "axes[2].imshow(output_layer[:,:,0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw7PFF5RQ_Jp",
        "colab_type": "text"
      },
      "source": [
        "## Actual Detector CenterNet\n",
        "Input of CenterNet:\n",
        "\n",
        "* Cropped image resized into 512x512x3\n",
        "\n",
        "Output of CenterNet:\n",
        "\n",
        "* Heatmap of center point 128x128x1\n",
        "\n",
        "* x-offset and y-offset of the center point inside the detected cell 128x128x2\n",
        "\n",
        "* width and height of the the detected object 128x128x2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvVTIOq3RRP4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "category_n=1\n",
        "output_layer_n=category_n+4\n",
        "output_height,output_width=128,128\n",
        "​\n",
        "def Datagen_centernet(filenames, batch_size):\n",
        "  x=[]\n",
        "  y=[]\n",
        "  \n",
        "  count=0\n",
        "​\n",
        "  while True:\n",
        "    for i in range(len(filenames)):\n",
        "      h_split=filenames[i][2]\n",
        "      w_split=filenames[i][3]\n",
        "      max_crop_ratio_h=1/h_split\n",
        "      max_crop_ratio_w=1/w_split\n",
        "      crop_ratio=np.random.uniform(0.5,1)\n",
        "      crop_ratio_h=max_crop_ratio_h*crop_ratio\n",
        "      crop_ratio_w=max_crop_ratio_w*crop_ratio\n",
        "      \n",
        "      with Image.open(filenames[i][0]) as f:\n",
        "        \n",
        "        #random crop\n",
        "        \n",
        "        pic_width,pic_height=f.size\n",
        "        f=np.asarray(f.convert('RGB'),dtype=np.uint8)\n",
        "        top_offset=np.random.randint(0,pic_height-int(crop_ratio_h*pic_height))\n",
        "        left_offset=np.random.randint(0,pic_width-int(crop_ratio_w*pic_width))\n",
        "        bottom_offset=top_offset+int(crop_ratio_h*pic_height)\n",
        "        right_offset=left_offset+int(crop_ratio_w*pic_width)\n",
        "        f=cv2.resize(f[top_offset:bottom_offset,left_offset:right_offset,:],(input_height,input_width))\n",
        "        x.append(f)      \n",
        "​\n",
        "      output_layer=np.zeros((output_height,output_width,(output_layer_n+category_n)))\n",
        "      for annotation in filenames[i][1]:\n",
        "        x_c=(annotation[1]-left_offset)*(output_width/int(crop_ratio_w*pic_width))\n",
        "        y_c=(annotation[2]-top_offset)*(output_height/int(crop_ratio_h*pic_height))\n",
        "        width=annotation[3]*(output_width/int(crop_ratio_w*pic_width))\n",
        "        height=annotation[4]*(output_height/int(crop_ratio_h*pic_height))\n",
        "        top=np.maximum(0,y_c-height/2)\n",
        "        left=np.maximum(0,x_c-width/2)\n",
        "        bottom=np.minimum(output_height,y_c+height/2)\n",
        "        right=np.minimum(output_width,x_c+width/2)\n",
        "          \n",
        "        if top>=(output_height-0.1) or left>=(output_width-0.1) or bottom<=0.1 or right<=0.1:#random crop(out of picture)\n",
        "          continue\n",
        "        width=right-left\n",
        "        height=bottom-top\n",
        "        x_c=(right+left)/2\n",
        "        y_c=(top+bottom)/2\n",
        "​\n",
        "        \n",
        "        category=0#not classify, just detect\n",
        "        heatmap=((np.exp(-(((np.arange(output_width)-x_c)/(width/10))**2)/2)).reshape(1,-1)\n",
        "                            *(np.exp(-(((np.arange(output_height)-y_c)/(height/10))**2)/2)).reshape(-1,1))\n",
        "        output_layer[:,:,category]=np.maximum(output_layer[:,:,category],heatmap[:,:])\n",
        "        output_layer[int(y_c//1),int(x_c//1),category_n+category]=1\n",
        "        output_layer[int(y_c//1),int(x_c//1),2*category_n]=y_c%1#height offset\n",
        "        output_layer[int(y_c//1),int(x_c//1),2*category_n+1]=x_c%1\n",
        "        output_layer[int(y_c//1),int(x_c//1),2*category_n+2]=height/output_height\n",
        "        output_layer[int(y_c//1),int(x_c//1),2*category_n+3]=width/output_width\n",
        "      y.append(output_layer)  \n",
        "    \n",
        "      count+=1\n",
        "      if count==batch_size:\n",
        "        x=np.array(x, dtype=np.float32)\n",
        "        y=np.array(y, dtype=np.float32)\n",
        "​\n",
        "        inputs=x/255\n",
        "        targets=y       \n",
        "        x=[]\n",
        "        y=[]\n",
        "        count=0\n",
        "        yield inputs, targets\n",
        "​\n",
        "def all_loss(y_true, y_pred):\n",
        "    mask=K.sign(y_true[...,2*category_n+2])\n",
        "    N=K.sum(mask)\n",
        "    alpha=2.\n",
        "    beta=4.\n",
        "​\n",
        "    heatmap_true_rate = K.flatten(y_true[...,:category_n])\n",
        "    heatmap_true = K.flatten(y_true[...,category_n:(2*category_n)])\n",
        "    heatmap_pred = K.flatten(y_pred[...,:category_n])\n",
        "    heatloss=-K.sum(heatmap_true*((1-heatmap_pred)**alpha)*K.log(heatmap_pred+1e-6)+(1-heatmap_true)*((1-heatmap_true_rate)**beta)*(heatmap_pred**alpha)*K.log(1-heatmap_pred+1e-6))\n",
        "    offsetloss=K.sum(K.abs(y_true[...,2*category_n]-y_pred[...,category_n]*mask)+K.abs(y_true[...,2*category_n+1]-y_pred[...,category_n+1]*mask))\n",
        "    sizeloss=K.sum(K.abs(y_true[...,2*category_n+2]-y_pred[...,category_n+2]*mask)+K.abs(y_true[...,2*category_n+3]-y_pred[...,category_n+3]*mask))\n",
        "    \n",
        "    all_loss=(heatloss+1.0*offsetloss+5.0*sizeloss)/N\n",
        "    return all_loss\n",
        "​\n",
        "def size_loss(y_true, y_pred):\n",
        "    mask=K.sign(y_true[...,2*category_n+2])\n",
        "    N=K.sum(mask)\n",
        "    sizeloss=K.sum(K.abs(y_true[...,2*category_n+2]-y_pred[...,category_n+2]*mask)+K.abs(y_true[...,2*category_n+3]-y_pred[...,category_n+3]*mask))\n",
        "    return (5*sizeloss)/N\n",
        "​\n",
        "def offset_loss(y_true, y_pred):\n",
        "    mask=K.sign(y_true[...,2*category_n+2])\n",
        "    N=K.sum(mask)\n",
        "    offsetloss=K.sum(K.abs(y_true[...,2*category_n]-y_pred[...,category_n]*mask)+K.abs(y_true[...,2*category_n+1]-y_pred[...,category_n+1]*mask))\n",
        "    return (offsetloss)/N\n",
        "  \n",
        "def heatmap_loss(y_true, y_pred):\n",
        "    mask=K.sign(y_true[...,2*category_n+2])\n",
        "    N=K.sum(mask)\n",
        "    alpha=2.\n",
        "    beta=4.\n",
        "​\n",
        "    heatmap_true_rate = K.flatten(y_true[...,:category_n])\n",
        "    heatmap_true = K.flatten(y_true[...,category_n:(2*category_n)])\n",
        "    heatmap_pred = K.flatten(y_pred[...,:category_n])\n",
        "    heatloss=-K.sum(heatmap_true*((1-heatmap_pred)**alpha)*K.log(heatmap_pred+1e-6)+(1-heatmap_true)*((1-heatmap_true_rate)**beta)*(heatmap_pred**alpha)*K.log(1-heatmap_pred+1e-6))\n",
        "    return heatloss/N\n",
        "​\n",
        "  \n",
        "def model_fit_centernet(model,train_list,cv_list,n_epoch,batch_size=32):\n",
        "    hist = model.fit_generator(\n",
        "        Datagen_centernet(train_list,batch_size),\n",
        "        steps_per_epoch = len(train_list) // batch_size,\n",
        "        epochs = n_epoch,\n",
        "        validation_data=Datagen_centernet(cv_list,batch_size),\n",
        "        validation_steps = len(cv_list) // batch_size,\n",
        "        callbacks = [lr_schedule],#early_stopping, reduce_lr, model_checkpoint],\n",
        "        shuffle = True,\n",
        "        verbose = 1\n",
        "    )\n",
        "    return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWByF5txRX6M",
        "colab_type": "text"
      },
      "source": [
        "Instance of the detector "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_VUmQGVRdjX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "model=create_model(input_shape=(input_height,input_width,3),size_detection_mode=False)\n",
        "​\n",
        "def lrs(epoch):\n",
        "    lr = 0.001\n",
        "    if epoch >= 20: lr = 0.0002\n",
        "    return lr\n",
        "​\n",
        "lr_schedule = LearningRateScheduler(lrs)\n",
        "​\n",
        "\"\"\"\n",
        "​\n",
        "# EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta=0, patience = 60, verbose = 1)\n",
        "# ModelCheckpoint\n",
        "weights_dir = '/model_2/'\n",
        "​\n",
        "if os.path.exists(weights_dir) == False:os.mkdir(weights_dir)\n",
        "model_checkpoint = ModelCheckpoint(weights_dir + \"val_loss{val_loss:.3f}.hdf5\", monitor = 'val_loss', verbose = 1,\n",
        "                                      save_best_only = True, save_weights_only = True, period = 3)\n",
        "# reduce learning rate\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 10, verbose = 1)\n",
        "\"\"\"\n",
        "model.load_weights('final_weights_step1.h5',by_name=True, skip_mismatch=True)\n",
        "​\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8nOaAWCRkTa",
        "colab_type": "text"
      },
      "source": [
        "train begins"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBfzZdXARhhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_list, cv_list = train_test_split(annotation_list_train_w_split, random_state = 111,test_size = 0.2)#stratified split is better\n",
        "​\n",
        "learning_rate=0.001\n",
        "n_epoch=30\n",
        "batch_size=32\n",
        "model.compile(loss=all_loss, optimizer=Adam(lr=learning_rate), metrics=[heatmap_loss,size_loss,offset_loss])\n",
        "hist = model_fit_centernet(model,train_list,cv_list,n_epoch,batch_size)\n",
        "​\n",
        "model.save_weights('final_weights_step2.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82kTNOD0R5J6",
        "colab_type": "text"
      },
      "source": [
        "split pictures into several parts using the results in step1, then run CenterNet for each splitted picture. After then, integrate them and run NMS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9Bw0263R56M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_in_h=512\n",
        "pred_in_w=512\n",
        "pred_out_h=int(pred_in_h/4)\n",
        "pred_out_w=int(pred_in_w/4)\n",
        "​\n",
        "for i in np.arange(0,1):\n",
        "  img = np.asarray(Image.open(cv_list[i][0]).resize((pred_in_w,pred_in_h)).convert('RGB'))\n",
        "  predict=model.predict((img.reshape(1,pred_in_h,pred_in_w,3))/255).reshape(pred_out_h,pred_out_w,(category_n+4))\n",
        "  heatmap=predict[:,:,0]\n",
        "​\n",
        "  fig, axes = plt.subplots(1, 2,figsize=(15,15))\n",
        "  axes[0].set_axis_off()\n",
        "  axes[0].imshow(img)\n",
        "  axes[1].set_axis_off()\n",
        "  axes[1].imshow(heatmap)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1G9U8QESzrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image, ImageDraw\n",
        "​\n",
        "def NMS_all(predicts,category_n,score_thresh,iou_thresh):\n",
        "  y_c=predicts[...,category_n]+np.arange(pred_out_h).reshape(-1,1)\n",
        "  x_c=predicts[...,category_n+1]+np.arange(pred_out_w).reshape(1,-1)\n",
        "  height=predicts[...,category_n+2]*pred_out_h\n",
        "  width=predicts[...,category_n+3]*pred_out_w\n",
        "​\n",
        "  count=0\n",
        "  for category in range(category_n):\n",
        "    predict=predicts[...,category]\n",
        "    mask=(predict>score_thresh)\n",
        "    #print(\"box_num\",np.sum(mask))\n",
        "    if mask.all==False:\n",
        "      continue\n",
        "    box_and_score=NMS(predict[mask],y_c[mask],x_c[mask],height[mask],width[mask],iou_thresh)\n",
        "    box_and_score=np.insert(box_and_score,0,category,axis=1)#category,score,top,left,bottom,right\n",
        "    if count==0:\n",
        "      box_and_score_all=box_and_score\n",
        "    else:\n",
        "      box_and_score_all=np.concatenate((box_and_score_all,box_and_score),axis=0)\n",
        "    count+=1\n",
        "  score_sort=np.argsort(box_and_score_all[:,1])[::-1]\n",
        "  box_and_score_all=box_and_score_all[score_sort]\n",
        "  #print(box_and_score_all)\n",
        "​\n",
        " \n",
        "  _,unique_idx=np.unique(box_and_score_all[:,2],return_index=True)\n",
        "  #print(unique_idx)\n",
        "  return box_and_score_all[sorted(unique_idx)]\n",
        "  \n",
        "def NMS(score,y_c,x_c,height,width,iou_thresh,merge_mode=False):\n",
        "  if merge_mode:\n",
        "    score=score\n",
        "    top=y_c\n",
        "    left=x_c\n",
        "    bottom=height\n",
        "    right=width\n",
        "  else:\n",
        "    #flatten\n",
        "    score=score.reshape(-1)\n",
        "    y_c=y_c.reshape(-1)\n",
        "    x_c=x_c.reshape(-1)\n",
        "    height=height.reshape(-1)\n",
        "    width=width.reshape(-1)\n",
        "    size=height*width\n",
        "    \n",
        "    \n",
        "    top=y_c-height/2\n",
        "    left=x_c-width/2\n",
        "    bottom=y_c+height/2\n",
        "    right=x_c+width/2\n",
        "    \n",
        "    inside_pic=(top>0)*(left>0)*(bottom<pred_out_h)*(right<pred_out_w)\n",
        "    outside_pic=len(inside_pic)-np.sum(inside_pic)\n",
        "    #if outside_pic>0:\n",
        "    #  print(\"{} boxes are out of picture\".format(outside_pic))\n",
        "    normal_size=(size<(np.mean(size)*10))*(size>(np.mean(size)/10))\n",
        "    score=score[inside_pic*normal_size]\n",
        "    top=top[inside_pic*normal_size]\n",
        "    left=left[inside_pic*normal_size]\n",
        "    bottom=bottom[inside_pic*normal_size]\n",
        "    right=right[inside_pic*normal_size]\n",
        "  \n",
        "​\n",
        "    \n",
        "​\n",
        "  #sort  \n",
        "  score_sort=np.argsort(score)[::-1]\n",
        "  score=score[score_sort]  \n",
        "  top=top[score_sort]\n",
        "  left=left[score_sort]\n",
        "  bottom=bottom[score_sort]\n",
        "  right=right[score_sort]\n",
        "  \n",
        "  area=((bottom-top)*(right-left))\n",
        "  \n",
        "  boxes=np.concatenate((score.reshape(-1,1),top.reshape(-1,1),left.reshape(-1,1),bottom.reshape(-1,1),right.reshape(-1,1)),axis=1)\n",
        "  \n",
        "  box_idx=np.arange(len(top))\n",
        "  alive_box=[]\n",
        "  while len(box_idx)>0:\n",
        "  \n",
        "    alive_box.append(box_idx[0])\n",
        "    \n",
        "    y1=np.maximum(top[0],top)\n",
        "    x1=np.maximum(left[0],left)\n",
        "    y2=np.minimum(bottom[0],bottom)\n",
        "    x2=np.minimum(right[0],right)\n",
        "    \n",
        "    cross_h=np.maximum(0,y2-y1)\n",
        "    cross_w=np.maximum(0,x2-x1)\n",
        "    still_alive=(((cross_h*cross_w)/area[0])<iou_thresh)\n",
        "    if np.sum(still_alive)==len(box_idx):\n",
        "      print(\"error\")\n",
        "      print(np.max((cross_h*cross_w)),area[0])\n",
        "    top=top[still_alive]\n",
        "    left=left[still_alive]\n",
        "    bottom=bottom[still_alive]\n",
        "    right=right[still_alive]\n",
        "    area=area[still_alive]\n",
        "    box_idx=box_idx[still_alive]\n",
        "  return boxes[alive_box]#score,top,left,bottom,right\n",
        "​\n",
        "​\n",
        "​\n",
        "def draw_rectangle(box_and_score,img,color):\n",
        "  number_of_rect=np.minimum(500,len(box_and_score))\n",
        "  \n",
        "  for i in reversed(list(range(number_of_rect))):\n",
        "    top, left, bottom, right = box_and_score[i,:]\n",
        "​\n",
        "    \n",
        "    top = np.floor(top + 0.5).astype('int32')\n",
        "    left = np.floor(left + 0.5).astype('int32')\n",
        "    bottom = np.floor(bottom + 0.5).astype('int32')\n",
        "    right = np.floor(right + 0.5).astype('int32')\n",
        "    #label = '{} {:.2f}'.format(predicted_class, score)\n",
        "    #print(label)\n",
        "    #rectangle=np.array([[left,top],[left,bottom],[right,bottom],[right,top]])\n",
        "​\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    #label_size = draw.textsize(label)\n",
        "    #print(label_size)\n",
        "    \n",
        "    #if top - label_size[1] >= 0:\n",
        "    #  text_origin = np.array([left, top - label_size[1]])\n",
        "    #else:\n",
        "    #  text_origin = np.array([left, top + 1])\n",
        "    \n",
        "    thickness=4\n",
        "    if color==\"red\":\n",
        "      rect_color=(255, 0, 0)\n",
        "    elif color==\"blue\":\n",
        "      rect_color=(0, 0, 255)\n",
        "    else:\n",
        "      rect_color=(0, 0, 0)\n",
        "      \n",
        "    \n",
        "    if i==0:\n",
        "      thickness=4\n",
        "    for j in range(2*thickness):#薄いから何重にか描く\n",
        "      draw.rectangle([left + j, top + j, right - j, bottom - j],\n",
        "                    outline=rect_color)\n",
        "    #draw.rectangle(\n",
        "    #            [tuple(text_origin), tuple(text_origin + label_size)],\n",
        "    #            fill=(0, 0, 255))\n",
        "    #draw.text(text_origin, label, fill=(0, 0, 0))\n",
        "    \n",
        "  del draw\n",
        "  return img\n",
        "            \n",
        "  \n",
        "def check_iou_score(true_boxes,detected_boxes,iou_thresh):\n",
        "  iou_all=[]\n",
        "  for detected_box in detected_boxes:\n",
        "    y1=np.maximum(detected_box[0],true_boxes[:,0])\n",
        "    x1=np.maximum(detected_box[1],true_boxes[:,1])\n",
        "    y2=np.minimum(detected_box[2],true_boxes[:,2])\n",
        "    x2=np.minimum(detected_box[3],true_boxes[:,3])\n",
        "    \n",
        "    cross_section=np.maximum(0,y2-y1)*np.maximum(0,x2-x1)\n",
        "    all_area=(detected_box[2]-detected_box[0])*(detected_box[3]-detected_box[1])+(true_boxes[:,2]-true_boxes[:,0])*(true_boxes[:,3]-true_boxes[:,1])\n",
        "    iou=np.max(cross_section/(all_area-cross_section))\n",
        "    #argmax=np.argmax(cross_section/(all_area-cross_section))\n",
        "    iou_all.append(iou)\n",
        "  score=2*np.sum(iou_all)/(len(detected_boxes)+len(true_boxes))\n",
        "  print(\"score:{}\".format(np.round(score,3)))\n",
        "  return score\n",
        "​\n",
        "                \n",
        "​\n",
        "​\n",
        "​\n",
        "for i in np.arange(0,5):\n",
        "  #print(cv_list[i][2:])\n",
        "  img=Image.open(cv_list[i][0]).convert(\"RGB\")\n",
        "  width,height=img.size\n",
        "  predict=model.predict((np.asarray(img.resize((pred_in_w,pred_in_h))).reshape(1,pred_in_h,pred_in_w,3))/255).reshape(pred_out_h,pred_out_w,(category_n+4))\n",
        "  \n",
        "  box_and_score=NMS_all(predict,category_n,score_thresh=0.3,iou_thresh=0.4)\n",
        "​\n",
        "  #print(\"after NMS\",len(box_and_score))\n",
        "  if len(box_and_score)==0:\n",
        "    continue\n",
        "​\n",
        "  true_boxes=cv_list[i][1][:,1:]#c_x,c_y,width_height\n",
        "  top=true_boxes[:,1:2]-true_boxes[:,3:4]/2\n",
        "  left=true_boxes[:,0:1]-true_boxes[:,2:3]/2\n",
        "  bottom=top+true_boxes[:,3:4]\n",
        "  right=left+true_boxes[:,2:3]\n",
        "  true_boxes=np.concatenate((top,left,bottom,right),axis=1)\n",
        "    \n",
        "  heatmap=predict[:,:,0]\n",
        " \n",
        "  print_w, print_h = img.size\n",
        "  #resize predocted box to original size\n",
        "  box_and_score=box_and_score*[1,1,print_h/pred_out_h,print_w/pred_out_w,print_h/pred_out_h,print_w/pred_out_w]\n",
        "  check_iou_score(true_boxes,box_and_score[:,2:],iou_thresh=0.5)\n",
        "  img=draw_rectangle(box_and_score[:,2:],img,\"red\")\n",
        "  img=draw_rectangle(true_boxes,img,\"blue\")\n",
        "  \n",
        "  fig, axes = plt.subplots(1, 2,figsize=(15,15))\n",
        "  #axes[0].set_axis_off()\n",
        "  axes[0].imshow(img)\n",
        "  #axes[1].set_axis_off()\n",
        "  axes[1].imshow(heatmap)#, cmap='gray')\n",
        "  #axes[2].set_axis_off()\n",
        "  #axes[2].imshow(heatmap_1)#, cmap='gray')\n",
        "  plt.show()\n",
        "​\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2-K41DlS5Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_and_detect(model,img,height_split_recommended,width_split_recommended,score_thresh=0.3,iou_thresh=0.4):\n",
        "  width,height=img.size\n",
        "  pred_in_w,pred_in_h=512,512\n",
        "  pred_out_w,pred_out_h=128,128\n",
        "  category_n=1\n",
        "  maxlap=0.5\n",
        "  height_split=int(-(-height_split_recommended//1)+1)\n",
        "  width_split=int(-(-width_split_recommended//1)+1)\n",
        "  height_lap=(height_split-height_split_recommended)/(height_split-1)\n",
        "  height_lap=np.minimum(maxlap,height_lap)\n",
        "  width_lap=(width_split-width_split_recommended)/(width_split-1)\n",
        "  width_lap=np.minimum(maxlap,width_lap)\n",
        "​\n",
        "  if height>width:\n",
        "    crop_size=int((height)/(height_split-(height_split-1)*height_lap))#crop_height and width\n",
        "    if crop_size>=width:\n",
        "      crop_size=width\n",
        "      stride=int((crop_size*height_split-height)/(height_split-1))\n",
        "      top_list=[i*stride for i in range(height_split-1)]+[height-crop_size]\n",
        "      left_list=[0]\n",
        "    else:\n",
        "      stride=int((crop_size*height_split-height)/(height_split-1))\n",
        "      top_list=[i*stride for i in range(height_split-1)]+[height-crop_size]\n",
        "      width_split=-(-width//crop_size)\n",
        "      stride=int((crop_size*width_split-width)/(width_split-1))\n",
        "      left_list=[i*stride for i in range(width_split-1)]+[width-crop_size]\n",
        "​\n",
        "  else:\n",
        "    crop_size=int((width)/(width_split-(width_split-1)*width_lap))#crop_height and width\n",
        "    if crop_size>=height:\n",
        "      crop_size=height\n",
        "      stride=int((crop_size*width_split-width)/(width_split-1))\n",
        "      left_list=[i*stride for i in range(width_split-1)]+[width-crop_size]\n",
        "      top_list=[0]\n",
        "    else:\n",
        "      stride=int((crop_size*width_split-width)/(width_split-1))\n",
        "      left_list=[i*stride for i in range(width_split-1)]+[width-crop_size]\n",
        "      height_split=-(-height//crop_size)\n",
        "      stride=int((crop_size*height_split-height)/(height_split-1))\n",
        "      top_list=[i*stride for i in range(height_split-1)]+[height-crop_size]\n",
        "  \n",
        "  count=0\n",
        "​\n",
        "  for top_offset in top_list:\n",
        "    for left_offset in left_list:\n",
        "      img_crop = img.crop((left_offset, top_offset, left_offset+crop_size, top_offset+crop_size))\n",
        "      predict=model.predict((np.asarray(img_crop.resize((pred_in_w,pred_in_h))).reshape(1,pred_in_h,pred_in_w,3))/255).reshape(pred_out_h,pred_out_w,(category_n+4))\n",
        "  \n",
        "      box_and_score=NMS_all(predict,category_n,score_thresh,iou_thresh)#category,score,top,left,bottom,right\n",
        "      \n",
        "      #print(\"after NMS\",len(box_and_score))\n",
        "      if len(box_and_score)==0:\n",
        "        continue\n",
        "      #reshape and offset\n",
        "      box_and_score=box_and_score*[1,1,crop_size/pred_out_h,crop_size/pred_out_w,crop_size/pred_out_h,crop_size/pred_out_w]+np.array([0,0,top_offset,left_offset,top_offset,left_offset])\n",
        "      \n",
        "      if count==0:\n",
        "        box_and_score_all=box_and_score\n",
        "      else:\n",
        "        box_and_score_all=np.concatenate((box_and_score_all,box_and_score),axis=0)\n",
        "      count+=1\n",
        "  #print(\"all_box_num:\",len(box_and_score_all))\n",
        "  #print(box_and_score_all[:10,:],np.min(box_and_score_all[:,2:]))\n",
        "  if count==0:\n",
        "    box_and_score_all=[]\n",
        "  else:\n",
        "    score=box_and_score_all[:,1]\n",
        "    y_c=(box_and_score_all[:,2]+box_and_score_all[:,4])/2\n",
        "    x_c=(box_and_score_all[:,3]+box_and_score_all[:,5])/2\n",
        "    height=-box_and_score_all[:,2]+box_and_score_all[:,4]\n",
        "    width=-box_and_score_all[:,3]+box_and_score_all[:,5]\n",
        "    #print(np.min(height),np.min(width))\n",
        "    box_and_score_all=NMS(box_and_score_all[:,1],box_and_score_all[:,2],box_and_score_all[:,3],box_and_score_all[:,4],box_and_score_all[:,5],iou_thresh=0.5,merge_mode=True)\n",
        "  return box_and_score_all\n",
        "​\n",
        "​\n",
        "print(\"test run. 5 image\")\n",
        "all_iou_score=[]\n",
        "for i in np.arange(0,5):\n",
        "  img=Image.open(cv_list[i][0]).convert(\"RGB\")\n",
        "  box_and_score_all=split_and_detect(model,img,cv_list[i][2],cv_list[i][3],score_thresh=0.3,iou_thresh=0.4)\n",
        "  if len(box_and_score_all)==0:\n",
        "    print(\"no box found\")\n",
        "    continue\n",
        "  true_boxes=cv_list[i][1][:,1:]#c_x,c_y,width_height\n",
        "  top=true_boxes[:,1:2]-true_boxes[:,3:4]/2\n",
        "  left=true_boxes[:,0:1]-true_boxes[:,2:3]/2\n",
        "  bottom=top+true_boxes[:,3:4]\n",
        "  right=left+true_boxes[:,2:3]\n",
        "  true_boxes=np.concatenate((top,left,bottom,right),axis=1)\n",
        "​\n",
        "  \n",
        "​\n",
        " \n",
        "  print_w, print_h = img.size\n",
        "  iou_score=check_iou_score(true_boxes,box_and_score_all[:,1:],iou_thresh=0.5)\n",
        "  all_iou_score.append(iou_score)\n",
        "  \"\"\"\n",
        "  img=draw_rectangle(box_and_score_all[:,1:],img,\"red\")\n",
        "  img=draw_rectangle(true_boxes,img,\"blue\")\n",
        "  \n",
        "  fig, axes = plt.subplots(1, 2,figsize=(15,15))\n",
        "  #axes[0].set_axis_off()\n",
        "  axes[0].imshow(img)\n",
        "  #axes[1].set_axis_off()\n",
        "  axes[1].imshow(heatmap)#, cmap='gray')\n",
        "​\n",
        "  plt.show()\n",
        "  \"\"\"\n",
        "print(\"average_score:\",np.mean(all_iou_score))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqHfQUa2S9kI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uy_XTW0TBZl",
        "colab_type": "text"
      },
      "source": [
        "## Classify "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7etBhV-TDKx",
        "colab_type": "text"
      },
      "source": [
        "### Crop and resize "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2vtfe37TFrn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "count=0\n",
        "crop_dir=\"/crop_letter/\"\n",
        "if os.path.exists(crop_dir) == False:os.mkdir(crop_dir)\n",
        "​\n",
        "train_input=[]\n",
        "pic_count=0\n",
        "for ann_pic in tqdm(annotation_list_train):\n",
        "  pic_count+=1\n",
        "  with Image.open(ann_pic[0]) as img:\n",
        "    for ann in ann_pic[1]:#cat,center_x,center_y,width,height for each picture\n",
        "      cat=ann[0]\n",
        "      c_x=ann[1]\n",
        "      c_y=ann[2]\n",
        "      width=ann[3]\n",
        "      height=ann[4]\n",
        "      save_dir=crop_dir+str(count)+\".jpg\"\n",
        "      img.crop((int(c_x-width/2),int(c_y-height/2),int(c_x+width/2),int(c_y+height/2))).save(save_dir)\n",
        "      train_input.append([save_dir,cat])\n",
        "      count+=1\n",
        "                 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJDrwQYGTHDK",
        "colab_type": "text"
      },
      "source": [
        "### Classification CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLaa-mbYTK4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_unicode_translation=pd.read_csv(\"../input/unicode_translation.csv\")\n",
        "unicode=df_unicode_translation[\"Unicode\"].values\n",
        "char=df_unicode_translation[\"char\"].values\n",
        "dict_translation={unicode[i]:char[i] for i in range(len(unicode))}\n",
        "​\n",
        "i=0\n",
        "img = np.asarray(Image.open(train_input[i][0]).resize((32,32)).convert('RGB'))\n",
        "name = dict_translation[inv_dict_cat[str(train_input[i][1])]]\n",
        "print(name)\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQZOqAFKTLwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_height,input_width=32,32\n",
        "​\n",
        "def Datagen_for_classification(filenames, batch_size, is_train=True,random_crop=True):\n",
        "  x=[]\n",
        "  y=[]\n",
        "  \n",
        "  count=0\n",
        "​\n",
        "  while True:\n",
        "    for i in range(len(filenames)):\n",
        "      if random_crop:\n",
        "        crop_ratio=np.random.uniform(0.8,1)\n",
        "      else:\n",
        "        crop_ratio=1\n",
        "      with Image.open(filenames[i][0]) as f:\n",
        "        \n",
        "        #random crop\n",
        "        if random_crop and is_train:\n",
        "          pic_width,pic_height=f.size\n",
        "          f=np.asarray(f.convert('RGB'),dtype=np.uint8)\n",
        "          top_offset=np.random.randint(0,pic_height-int(crop_ratio*pic_height))\n",
        "          left_offset=np.random.randint(0,pic_width-int(crop_ratio*pic_width))\n",
        "          bottom_offset=top_offset+int(crop_ratio*pic_height)\n",
        "          right_offset=left_offset+int(crop_ratio*pic_width)\n",
        "          f=cv2.resize(f[top_offset:bottom_offset,left_offset:right_offset,:],(input_height,input_width))\n",
        "        else:\n",
        "          f=f.resize((input_width, input_height))\n",
        "          f=np.asarray(f.convert('RGB'),dtype=np.uint8)          \n",
        "        x.append(f)\n",
        "      \n",
        "        y.append(int(filenames[i][1]))\n",
        "      count+=1\n",
        "      if count==batch_size:\n",
        "        x=np.array(x, dtype=np.float32)\n",
        "        y=np.identity(len(dict_cat))[y].astype(np.float32)\n",
        "​\n",
        "        inputs=x/255\n",
        "        targets=y       \n",
        "        x=[]\n",
        "        y=[]\n",
        "        count=0\n",
        "        yield inputs, targets\n",
        "        \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYcEccOoRKL2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_classification_model(input_shape, n_category):\n",
        "    input_layer = Input(input_shape)#32\n",
        "    x=cbr(input_layer,64,3,1)\n",
        "    x=resblock(x,64)\n",
        "    x=resblock(x,64)\n",
        "    x=cbr(input_layer,128,3,2)#16\n",
        "    x=resblock(x,128)\n",
        "    x=resblock(x,128)\n",
        "    x=cbr(input_layer,256,3,2)#8\n",
        "    x=resblock(x,256)\n",
        "    x=resblock(x,256)\n",
        "    x=GlobalAveragePooling2D()(x)\n",
        "    x=Dropout(0.2)(x)\n",
        "    out=Dense(n_category,activation=\"softmax\")(x)\n",
        "    \n",
        "    classification_model=Model(input_layer, out)\n",
        "    \n",
        "    return classification_model\n",
        "      \n",
        "def model_fit_classification(model,train_list,cv_list,n_epoch,batch_size=32):\n",
        "    hist = model.fit_generator(\n",
        "        Datagen_for_classification(train_list,batch_size, is_train=True,random_crop=True),\n",
        "        steps_per_epoch = len(train_list) // batch_size,\n",
        "        epochs = n_epoch,\n",
        "        validation_data=Datagen_for_classification(cv_list,batch_size, is_train=False,random_crop=False),\n",
        "        validation_steps = len(cv_list) // batch_size,\n",
        "        #callbacks = [early_stopping, reduce_lr, model_checkpoint],\n",
        "        shuffle = True,\n",
        "        verbose = 1\n",
        "    )\n",
        "    return hist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SpyYt-KTPHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "K.clear_session()\n",
        "input_height,input_width=32,32\n",
        "model=create_classification_model(input_shape=(input_height,input_width,3),n_category=len(dict_cat))\n",
        "\n",
        "​\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm7NCyd3TRfq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_list, cv_list = train_test_split(train_input, random_state = 111,test_size = 0.2)\n",
        "learning_rate=0.005\n",
        "n_epoch=10\n",
        "batch_size=64\n",
        "​\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=learning_rate),metrics=[\"accuracy\"])\n",
        "hist = model_fit_classification(model,train_list,cv_list,n_epoch,batch_size)\n",
        "​\n",
        "model.save_weights('final_weights_step3.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey8yLjXRTUd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(3):\n",
        "  img = np.asarray(Image.open(train_input[i][0]).resize((32,32)).convert('RGB'))\n",
        "  predict=np.argmax(model.predict(img.reshape(1,32,32,3)/255),axis=1)[0]\n",
        "  name = dict_translation[inv_dict_cat[str(predict)]]\n",
        "  print(name)\n",
        "  plt.imshow(img)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6PeJynzTWe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5epgDanTYjP",
        "colab_type": "text"
      },
      "source": [
        "## Test + Submit "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrMKA-XKTbSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "​\n",
        "K.clear_session()\n",
        "print(\"loading models...\")\n",
        "model_1=create_model(input_shape=(512,512,3),size_detection_mode=True)\n",
        "#model_1.load_weights('final_weights_step1.h5')\n",
        "model_1.load_weights('final_weights_step1.hdf5')\n",
        "​\n",
        "model_2=create_model(input_shape=(512,512,3),size_detection_mode=False)\n",
        "model_2.load_weights('final_weights_step2.h5')\n",
        "​\n",
        "model_3=create_classification_model(input_shape=(32,32,3),n_category=len(dict_cat))\n",
        "model_3.load_weights('final_weights_step3.h5')\n",
        "​\n",
        "​\n",
        "def pipeline(i,print_img=False):\n",
        "  # model1: determine how to split image\n",
        "  if print_img: print(\"model 1\")\n",
        "  img = np.asarray(Image.open(id_test[i]).resize((512,512)).convert('RGB'))\n",
        "  predicted_size=model_1.predict(img.reshape(1,512,512,3)/255)\n",
        "  detect_num_h=aspect_ratio_pic_all_test[i]*np.exp(-predicted_size/2)\n",
        "  detect_num_w=detect_num_h/aspect_ratio_pic_all_test[i]\n",
        "  h_split_recommend=np.maximum(1,detect_num_h/base_detect_num_h)\n",
        "  w_split_recommend=np.maximum(1,detect_num_w/base_detect_num_w)\n",
        "  if print_img: print(\"recommended split_h:{}, split_w:{}\".format(h_split_recommend,w_split_recommend))\n",
        "​\n",
        "  # model2: detection\n",
        "  if print_img: print(\"model 2\")\n",
        "  img=Image.open(id_test[i]).convert(\"RGB\")\n",
        "  box_and_score_all=split_and_detect(model_2,img,h_split_recommend,w_split_recommend,score_thresh=0.3,iou_thresh=0.4)#output:score,top,left,bottom,right\n",
        "  if print_img: print(\"find {} boxes\".format(len(box_and_score_all)))\n",
        "  print_w, print_h = img.size\n",
        "  if (len(box_and_score_all)>0) and print_img: \n",
        "    img=draw_rectangle(box_and_score_all[:,1:],img,\"red\")\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "​\n",
        "  # model3: classification\n",
        "  count=0\n",
        "  if (len(box_and_score_all)>0):\n",
        "    for box in box_and_score_all[:,1:]:\n",
        "      top,left,bottom,right=box\n",
        "      img_letter=img.crop((int(left),int(top),int(right),int(bottom))).resize((32,32))\n",
        "      predict=(model_3.predict(np.asarray(img_letter).reshape(1,32,32,3)/255))\n",
        "      predict=np.argmax(predict,axis=1)[0]\n",
        "      code=inv_dict_cat[str(predict)]\n",
        "      c_x=int((left+right)/2)\n",
        "      c_y=int((top+bottom)/2)\n",
        "      if count==0:\n",
        "        ans=code+\" \"+str(c_x)+\" \"+str(c_y)\n",
        "      else:\n",
        "        ans=ans+\" \"+code+\" \"+str(c_x)+\" \"+str(c_y)\n",
        "      count+=1\n",
        "  else:\n",
        "    ans=\"\"\n",
        "  return ans\n",
        "​\n",
        "_=pipeline(0,print_img=True)\n",
        "​\n",
        "\n",
        "for i in tqdm(range(len(id_test))):\n",
        "  ans=pipeline(i,print_img=False)\n",
        "  df_submission.set_value(i, 'labels', ans)\n",
        "      \n",
        "df_submission.to_csv(\"submission.csv\",index=False)\n",
        "​"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRd9Cc43TcJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "df_submission.head()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}